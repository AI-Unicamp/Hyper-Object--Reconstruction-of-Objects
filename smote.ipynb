{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q h5py pandas matplotlib opencv-python torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952b6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "278ca104",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8, 6)\n",
    "\n",
    "# Global configuration\n",
    "N_BANDS_HSI = 61\n",
    "\n",
    "# Target resolution for each modality\n",
    "HSI_TARGET_H, HSI_TARGET_W = 1024, 1024   # HSI: 1024x1024x61\n",
    "RGB_TARGET_H, RGB_TARGET_W = 512, 512     # RGB:  512x512x3\n",
    "\n",
    "# GPU configuration (PyTorch)\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "except Exception:\n",
    "    torch = None\n",
    "    F = None\n",
    "    DEVICE = None\n",
    "\n",
    "USE_GPU_INTERP = (torch is not None) and (DEVICE.type == \"cuda\")\n",
    "USE_GPU_RESIZE = False  # set True to resize HSI/RGB on GPU (can require more VRAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513ec345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hsi_cube(path: str) -> np.ndarray:\n",
    "    \"\"\"Load HSI cube (expected: H,W,61) from an .h5 file.\"\"\"\n",
    "    try:\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if \"cube\" in f:\n",
    "                data = f[\"cube\"][:]\n",
    "            else:\n",
    "                keys = list(f.keys())\n",
    "                if len(keys) == 0:\n",
    "                    raise ValueError(f\"H5 file has no datasets: {path}\")\n",
    "                data = f[keys[0]][:]\n",
    "    except OSError as e:\n",
    "        raise OSError(f\"Error reading H5: {path}\\n{e}\")\n",
    "\n",
    "    if data.ndim != 3 or data.shape[-1] != N_BANDS_HSI:\n",
    "        raise ValueError(f\"Expected (H, W, {N_BANDS_HSI}), got {data.shape} in {path}\")\n",
    "\n",
    "    return data.astype(np.float32, copy=False)\n",
    "\n",
    "def ensure_hsi_size(cube: np.ndarray, target_h: int, target_w: int,\n",
    "                    use_gpu: bool = USE_GPU_RESIZE) -> np.ndarray:\n",
    "    \"\"\"Ensure HSI is target_h x target_w x 61 (resize if needed).\"\"\"\n",
    "    h, w, b = cube.shape\n",
    "    if (h, w) == (target_h, target_w):\n",
    "        return cube\n",
    "\n",
    "    if use_gpu and (torch is not None) and (F is not None) and (DEVICE is not None):\n",
    "        # H,W,C -> 1,C,H,W\n",
    "        t = torch.from_numpy(cube).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "        t = F.interpolate(t, size=(target_h, target_w), mode=\"bilinear\", align_corners=False)\n",
    "        out = t.squeeze(0).permute(1, 2, 0).contiguous().cpu().numpy()\n",
    "        return out.astype(np.float32, copy=False)\n",
    "\n",
    "    # CPU fallback (OpenCV, per-band)\n",
    "    bands = []\n",
    "    for i in range(b):\n",
    "        band = cube[:, :, i]\n",
    "        band_resized = cv2.resize(band, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "        bands.append(band_resized)\n",
    "    return np.stack(bands, axis=-1).astype(np.float32, copy=False)\n",
    "\n",
    "def ensure_rgb_size(rgb: np.ndarray, target_h: int, target_w: int,\n",
    "                    use_gpu: bool = USE_GPU_RESIZE) -> np.ndarray:\n",
    "    \"\"\"Ensure RGB is target_h x target_w x 3 (resize if needed).\"\"\"\n",
    "    h, w, c = rgb.shape\n",
    "    if (h, w) == (target_h, target_w):\n",
    "        return rgb\n",
    "\n",
    "    if use_gpu and (torch is not None) and (F is not None) and (DEVICE is not None):\n",
    "        # H,W,C -> 1,C,H,W\n",
    "        t = torch.from_numpy(rgb).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "        t = F.interpolate(t, size=(target_h, target_w), mode=\"bilinear\", align_corners=False)\n",
    "        out = t.squeeze(0).permute(1, 2, 0).contiguous().cpu().numpy()\n",
    "        return out.astype(np.float32, copy=False)\n",
    "\n",
    "    return cv2.resize(rgb, (target_w, target_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "def hsi_false_color(hsi_cube: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Create a false-color RGB visualization from 3 HSI bands.\"\"\"\n",
    "    b1, b2, b3 = 0, N_BANDS_HSI // 2, N_BANDS_HSI - 1\n",
    "    rgb = np.stack([hsi_cube[..., b1], hsi_cube[..., b2], hsi_cube[..., b3]], axis=-1)\n",
    "    rgb = rgb - rgb.min()\n",
    "    m = rgb.max()\n",
    "    if m > 0:\n",
    "        rgb = rgb / m\n",
    "    return rgb\n",
    "\n",
    "def smote_interpolate_pair(hsi1: np.ndarray, rgb1: np.ndarray,\n",
    "                           hsi2: np.ndarray, rgb2: np.ndarray,\n",
    "                           lam: float | None = None,\n",
    "                           use_gpu: bool = USE_GPU_INTERP) -> tuple[np.ndarray, np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Linear interpolation (SMOTE-like) between two HSI+RGB pairs:\n",
    "\n",
    "      x_syn = x1 + lam * (x2 - x1),  lam ~ U(0,1)\n",
    "\n",
    "    HSI and RGB are interpolated separately.\n",
    "    \"\"\"\n",
    "    if lam is None:\n",
    "        lam = float(np.random.rand())\n",
    "\n",
    "    if use_gpu and (torch is not None) and (DEVICE is not None):\n",
    "        lam_t = torch.tensor(lam, device=DEVICE, dtype=torch.float32)\n",
    "\n",
    "        h1 = torch.from_numpy(hsi1).to(DEVICE)\n",
    "        h2 = torch.from_numpy(hsi2).to(DEVICE)\n",
    "        r1 = torch.from_numpy(rgb1).to(DEVICE)\n",
    "        r2 = torch.from_numpy(rgb2).to(DEVICE)\n",
    "\n",
    "        h_syn = h1 + lam_t * (h2 - h1)\n",
    "        r_syn = r1 + lam_t * (r2 - r1)\n",
    "\n",
    "        return h_syn.detach().cpu().numpy(), r_syn.detach().cpu().numpy(), lam\n",
    "\n",
    "    hsi_syn = hsi1 + lam * (hsi2 - hsi1)\n",
    "    rgb_syn = rgb1 + lam * (rgb2 - rgb1)\n",
    "    return hsi_syn, rgb_syn, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bcd063",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_TRAIN_DIR = r\"/path/to/track2/train\"\n",
    "BASE_TEST_DIR  = r\"/path/to/track2/test-public\"\n",
    "\n",
    "# Output root for Data Augmentation (local)\n",
    "OUT_ROOT_DA = r\"./DataAugmentation\"\n",
    "\n",
    "OUT_TRAIN_HSI = os.path.join(OUT_ROOT_DA, \"train\", \"hsi_61\")\n",
    "OUT_TRAIN_RGB = os.path.join(OUT_ROOT_DA, \"train\", \"rgb_2\")\n",
    "OUT_TEST_HSI  = os.path.join(OUT_ROOT_DA, \"test-public\", \"hsi_61\")\n",
    "OUT_TEST_RGB  = os.path.join(OUT_ROOT_DA, \"test-public\", \"rgb_2\")\n",
    "\n",
    "for d in [OUT_TRAIN_HSI, OUT_TRAIN_RGB, OUT_TEST_HSI, OUT_TEST_RGB]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"BASE_TRAIN_DIR:\", BASE_TRAIN_DIR)\n",
    "print(\"BASE_TEST_DIR :\", BASE_TEST_DIR)\n",
    "print(\"OUT_ROOT_DA   :\", OUT_ROOT_DA)\n",
    "print(\"GPU device    :\", DEVICE if DEVICE is not None else \"torch not available\")\n",
    "print(\"USE_GPU_INTERP:\", USE_GPU_INTERP, \"| USE_GPU_RESIZE:\", USE_GPU_RESIZE)\n",
    "\n",
    "def ensure_local_copy(src_dir: str, dst_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Optional helper: copy a dataset folder to a local, stable path (useful for HDF5).\n",
    "    If dst_dir exists and is not empty, it is reused.\n",
    "    \"\"\"\n",
    "    src = Path(src_dir)\n",
    "    dst = Path(dst_dir)\n",
    "\n",
    "    if dst.exists() and any(dst.iterdir()):\n",
    "        print(f\"Local folder exists and is not empty: {dst}\")\n",
    "        print(\"-> Reusing existing copy.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Copying data from {src} to {dst} ...\")\n",
    "    if dst.exists():\n",
    "        shutil.rmtree(dst)\n",
    "    dst.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy contents of src_dir into dst_dir\n",
    "    for item in src.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.copytree(item, dst / item.name)\n",
    "        else:\n",
    "            shutil.copy2(item, dst / item.name)\n",
    "\n",
    "    print(\"Copy completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae436b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_labels_df(base_dir: str, split_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read hsi_61/*.h5 and find the RGB pair in rgb_2 with the same basename.\n",
    "    Extract:\n",
    "      - id (e.g., Category-1_a_0000)\n",
    "      - category_name (e.g., Category-1)\n",
    "      - label (e.g., 1)\n",
    "      - relative paths to files\n",
    "    \"\"\"\n",
    "    hsi_dir = os.path.join(base_dir, \"hsi_61\")\n",
    "    rgb_dir = os.path.join(base_dir, \"rgb_2\")\n",
    "\n",
    "    hsi_paths = sorted(glob.glob(os.path.join(hsi_dir, \"*.h5\")))\n",
    "    rows = []\n",
    "    missing_rgb = 0\n",
    "\n",
    "    for hsi_path in hsi_paths:\n",
    "        base = os.path.splitext(os.path.basename(hsi_path))[0]\n",
    "        rgb_path = None\n",
    "\n",
    "        for ext in [\"png\", \"jpg\", \"jpeg\", \"tif\", \"tiff\"]:\n",
    "            cand = os.path.join(rgb_dir, f\"{base}.{ext}\")\n",
    "            if os.path.exists(cand):\n",
    "                rgb_path = cand\n",
    "                break\n",
    "\n",
    "        if rgb_path is None:\n",
    "            missing_rgb += 1\n",
    "            continue\n",
    "\n",
    "        first_token = base.split(\"_\")[0]  # \"Category-1\"\n",
    "        category_name = first_token\n",
    "        try:\n",
    "            label_num = int(category_name.split(\"-\")[-1])\n",
    "        except Exception:\n",
    "            label_num = -1\n",
    "\n",
    "        rows.append({\n",
    "            \"split\": split_name,\n",
    "            \"id\": base,\n",
    "            \"category_name\": category_name,\n",
    "            \"label\": label_num,\n",
    "            \"hsi_relpath\": os.path.relpath(hsi_path, base_dir),\n",
    "            \"rgb_relpath\": os.path.relpath(rgb_path, base_dir),\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"{split_name}: {len(df)} HSI+RGB pairs; {missing_rgb} HSI files without RGB.\")\n",
    "    print(\"Label distribution:\")\n",
    "    print(df[\"label\"].value_counts().sort_index())\n",
    "    return df\n",
    "\n",
    "labels_train = build_labels_df(BASE_TRAIN_DIR, \"train\")\n",
    "labels_test  = build_labels_df(BASE_TEST_DIR,  \"test-public\")\n",
    "\n",
    "print(\"\\nLabel distribution (train):\")\n",
    "print(labels_train[\"label\"].value_counts().sort_index())\n",
    "print(\"\\nLabel distribution (test-public):\")\n",
    "print(labels_test[\"label\"].value_counts().sort_index())\n",
    "\n",
    "\n",
    "# Save metadata\n",
    "os.makedirs(OUT_ROOT_DA, exist_ok=True)\n",
    "LABELS_TRAIN_CSV = os.path.join(OUT_ROOT_DA, \"labels_train.csv\")\n",
    "LABELS_TEST_CSV  = os.path.join(OUT_ROOT_DA, \"labels_test-public.csv\")\n",
    "\n",
    "labels_train.to_csv(LABELS_TRAIN_CSV, index=False)\n",
    "labels_test.to_csv(LABELS_TEST_CSV,  index=False)\n",
    "\n",
    "print(\"\\nMetadata saved to:\")\n",
    "print(\" \", LABELS_TRAIN_CSV)\n",
    "print(\" \", LABELS_TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24b249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SMOTE plan to balance the 4 classes\n",
    "def build_smote_plan_for_split(labels_df: pd.DataFrame,\n",
    "                               split_name: str,\n",
    "                               random_state: int = 42) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build a DataFrame with columns:\n",
    "      split, label, id1, id2, synthetic_id\n",
    "\n",
    "    After generating all synthetic samples in this plan, class counts\n",
    "    should be balanced within the split.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    labels_df = labels_df[labels_df[\"label\"] >= 0].copy()\n",
    "    class_counts = labels_df[\"label\"].value_counts().sort_index()\n",
    "    if class_counts.empty:\n",
    "        raise ValueError(f\"No valid classes found for split {split_name}.\")\n",
    "\n",
    "    print(f\"\\n==== {split_name}: current class counts ====\")\n",
    "    print(class_counts)\n",
    "\n",
    "    max_count = int(class_counts.max())\n",
    "    print(f\"Most frequent class in {split_name}: {max_count} samples.\")\n",
    "\n",
    "    plan_rows = []\n",
    "\n",
    "    for label, count in class_counts.items():\n",
    "        needed = max_count - int(count)\n",
    "        if needed <= 0:\n",
    "            continue\n",
    "\n",
    "        df_c = labels_df[labels_df[\"label\"] == label]\n",
    "        ids = df_c[\"id\"].tolist()\n",
    "\n",
    "        if len(ids) < 2:\n",
    "            print(f\"Warning: label {label} in split {split_name} has < 2 samples; cannot do SMOTE-like interpolation.\")\n",
    "            continue\n",
    "\n",
    "        used_pairs = set()\n",
    "        attempts = 0\n",
    "\n",
    "        while needed > 0 and attempts < 10000:\n",
    "            id1, id2 = rng.choice(ids, size=2, replace=False)\n",
    "            pair_key = tuple(sorted((id1, id2)))\n",
    "            if pair_key in used_pairs:\n",
    "                attempts += 1\n",
    "                continue\n",
    "\n",
    "            used_pairs.add(pair_key)\n",
    "            synthetic_id = f\"{id1}_{id2}\"\n",
    "            plan_rows.append({\n",
    "                \"split\": split_name,\n",
    "                \"label\": int(label),\n",
    "                \"id1\": id1,\n",
    "                \"id2\": id2,\n",
    "                \"synthetic_id\": synthetic_id,\n",
    "            })\n",
    "            needed -= 1\n",
    "            attempts = 0\n",
    "\n",
    "        if needed > 0:\n",
    "            print(f\"Warning: could not plan all synthetic samples for label {label} in {split_name}. Missing {needed}.\")\n",
    "\n",
    "    plan_df = pd.DataFrame(plan_rows)\n",
    "    print(f\"\\nPlan for {split_name}: {len(plan_df)} synthetic samples to generate.\")\n",
    "    return plan_df\n",
    "\n",
    "# Reload metadata from disk\n",
    "labels_train = pd.read_csv(LABELS_TRAIN_CSV)\n",
    "labels_test  = pd.read_csv(LABELS_TEST_CSV)\n",
    "\n",
    "plan_train = build_smote_plan_for_split(labels_train, \"train\")\n",
    "plan_test  = build_smote_plan_for_split(labels_test,  \"test-public\")\n",
    "\n",
    "\n",
    "smote_plan = pd.concat([plan_train, plan_test], ignore_index=True)\n",
    "\n",
    "SMOTE_PLAN_CSV = os.path.join(OUT_ROOT_DA, \"smote_plan.csv\")\n",
    "smote_plan.to_csv(SMOTE_PLAN_CSV, index=False)\n",
    "\n",
    "print(\"\\nGlobal SMOTE plan saved to:\")\n",
    "print(\" \", SMOTE_PLAN_CSV)\n",
    "\n",
    "print(\"\\nPlan summary (by split):\")\n",
    "print(smote_plan.groupby([\"split\", \"label\"]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c05a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate one synthetic pair (HSI + RGB) per call\n",
    "def generate_next_synthetic_from_plan(\n",
    "    plan_path: str,\n",
    "    labels_train_path: str,\n",
    "    labels_test_path: str,\n",
    "    base_train_dir: str,\n",
    "    base_test_dir: str,\n",
    "    out_root_da: str,\n",
    "    split_filter: str | None = None,   # \"train\", \"test-public\", or None (both)\n",
    "    show_plot: bool = True,\n",
    "    use_gpu_interp: bool = USE_GPU_INTERP\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Read smote_plan.csv, find the first row whose synthetic files do not exist,\n",
    "    generate the interpolated HSI+RGB pair (SMOTE-like), and save it.\n",
    "\n",
    "    Generates only ONE pair per call.\n",
    "    \"\"\"\n",
    "    plan_df = pd.read_csv(plan_path)\n",
    "\n",
    "    if split_filter is not None:\n",
    "        plan_df = plan_df[plan_df[\"split\"] == split_filter].copy()\n",
    "        if plan_df.empty:\n",
    "            print(f\"No entries in the plan for split={split_filter}.\")\n",
    "            return\n",
    "\n",
    "    labels_train = pd.read_csv(labels_train_path)\n",
    "    labels_test  = pd.read_csv(labels_test_path)\n",
    "\n",
    "    labels_by_split = {\n",
    "        \"train\": labels_train,\n",
    "        \"test-public\": labels_test,\n",
    "    }\n",
    "    base_by_split = {\n",
    "        \"train\": base_train_dir,\n",
    "        \"test-public\": base_test_dir,\n",
    "    }\n",
    "\n",
    "    # Ensure output directories exist\n",
    "    out_dirs = {}\n",
    "    for split in [\"train\", \"test-public\"]:\n",
    "        out_hsi = os.path.join(out_root_da, split, \"hsi_61\")\n",
    "        out_rgb = os.path.join(out_root_da, split, \"rgb_2\")\n",
    "        os.makedirs(out_hsi, exist_ok=True)\n",
    "        os.makedirs(out_rgb, exist_ok=True)\n",
    "        out_dirs[split] = {\"hsi\": out_hsi, \"rgb\": out_rgb}\n",
    "\n",
    "    # Find next pending entry\n",
    "    for _, row in plan_df.iterrows():\n",
    "        split = row[\"split\"]\n",
    "        synthetic_id = row[\"synthetic_id\"]\n",
    "\n",
    "        out_hsi_path = os.path.join(out_dirs[split][\"hsi\"], synthetic_id + \".h5\")\n",
    "        out_rgb_path = os.path.join(out_dirs[split][\"rgb\"], synthetic_id + \".png\")\n",
    "\n",
    "        if os.path.exists(out_hsi_path) and os.path.exists(out_rgb_path):\n",
    "            continue\n",
    "\n",
    "        labels_df = labels_by_split[split]\n",
    "        base_dir  = base_by_split[split]\n",
    "\n",
    "        id1 = row[\"id1\"]\n",
    "        id2 = row[\"id2\"]\n",
    "\n",
    "        row1 = labels_df[labels_df[\"id\"] == id1]\n",
    "        row2 = labels_df[labels_df[\"id\"] == id2]\n",
    "        if row1.empty or row2.empty:\n",
    "            print(f\"Warning: missing metadata for {id1} or {id2} in split {split}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        row1 = row1.iloc[0]\n",
    "        row2 = row2.iloc[0]\n",
    "\n",
    "        hsi1_path = os.path.join(base_dir, row1[\"hsi_relpath\"])\n",
    "        rgb1_path = os.path.join(base_dir, row1[\"rgb_relpath\"])\n",
    "        hsi2_path = os.path.join(base_dir, row2[\"hsi_relpath\"])\n",
    "        rgb2_path = os.path.join(base_dir, row2[\"rgb_relpath\"])\n",
    "\n",
    "        # Load original HSI and RGB\n",
    "        hsi1 = ensure_hsi_size(load_hsi_cube(hsi1_path), HSI_TARGET_H, HSI_TARGET_W)\n",
    "        hsi1 = hsi1 - hsi1.min()\n",
    "        m1 = hsi1.max()\n",
    "        if m1 > 0:\n",
    "            hsi1 = hsi1 / m1\n",
    "\n",
    "        hsi2 = ensure_hsi_size(load_hsi_cube(hsi2_path), HSI_TARGET_H, HSI_TARGET_W)\n",
    "        hsi2 = hsi2 - hsi2.min()\n",
    "        m2 = hsi2.max()\n",
    "        if m2 > 0:\n",
    "            hsi2 = hsi2 / m2\n",
    "\n",
    "        rgb1 = cv2.imread(rgb1_path, cv2.IMREAD_COLOR)\n",
    "        if rgb1 is None:\n",
    "            print(f\"Failed to read RGB: {rgb1_path}\")\n",
    "            return\n",
    "        rgb1 = cv2.cvtColor(rgb1, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        rgb1 = ensure_rgb_size(rgb1, RGB_TARGET_H, RGB_TARGET_W)\n",
    "\n",
    "        rgb2 = cv2.imread(rgb2_path, cv2.IMREAD_COLOR)\n",
    "        if rgb2 is None:\n",
    "            print(f\"Failed to read RGB: {rgb2_path}\")\n",
    "            return\n",
    "        rgb2 = cv2.cvtColor(rgb2, cv2.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "        rgb2 = ensure_rgb_size(rgb2, RGB_TARGET_H, RGB_TARGET_W)\n",
    "\n",
    "        # SMOTE-like interpolation (optionally on GPU)\n",
    "        hsi_syn, rgb_syn, lam = smote_interpolate_pair(hsi1, rgb1, hsi2, rgb2, use_gpu=use_gpu_interp)\n",
    "\n",
    "        # Save synthetic HSI to .h5\n",
    "        with h5py.File(out_hsi_path, \"w\") as f:\n",
    "            f.create_dataset(\"cube\", data=hsi_syn.astype(np.float32, copy=False))\n",
    "\n",
    "        # Save synthetic RGB to .png\n",
    "        rgb_uint8 = (np.clip(rgb_syn, 0.0, 1.0) * 255).astype(np.uint8)\n",
    "        cv2.imwrite(out_rgb_path, cv2.cvtColor(rgb_uint8, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        print(f\"\\nGenerated synthetic {synthetic_id}\")\n",
    "        print(f\"  split = {split}, label = {row['label']}, lambda = {lam:.3f}\")\n",
    "        print(f\"  HSI saved to: {out_hsi_path}\")\n",
    "        print(f\"  RGB saved to: {out_rgb_path}\")\n",
    "\n",
    "        if show_plot:\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.title(f\"{id1} (RGB)\")\n",
    "            plt.imshow(rgb1)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.title(f\"{id2} (RGB)\")\n",
    "            plt.imshow(rgb2)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.title(f\"{synthetic_id} (SMOTE-like)\")\n",
    "            plt.imshow(rgb_syn)\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # Cleanup\n",
    "        del hsi1, hsi2, rgb1, rgb2, hsi_syn, rgb_syn\n",
    "        gc.collect()\n",
    "        return\n",
    "\n",
    "    print(\"No pending entries in the plan (all synthetic samples were generated).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate 1 synthetic sample at a time\n",
    "\n",
    "generate_next_synthetic_from_plan(\n",
    "    plan_path=SMOTE_PLAN_CSV,\n",
    "    labels_train_path=LABELS_TRAIN_CSV,\n",
    "    labels_test_path=LABELS_TEST_CSV,\n",
    "    base_train_dir=BASE_TRAIN_DIR,\n",
    "    base_test_dir=BASE_TEST_DIR,\n",
    "    out_root_da=OUT_ROOT_DA,\n",
    "    split_filter=None,     # or \"train\" / \"test-public\"\n",
    "    show_plot=True,\n",
    "    use_gpu_interp=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-smote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
